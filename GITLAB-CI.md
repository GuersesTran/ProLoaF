# Load forecasting on GitLab CI/CD

This Repository utilizes GitLab's CI/CD to train and evaluate RNN-Models in the cloud. Training has to be initiated manually by a user via [api](https://docs.gitlab.com/ee/api/pipelines.html#create-a-new-pipeline) or [web call](https://git.rwth-aachen.de/acs/public/automation/plf/plf-training/-/pipelines/new).

Additionally, after every commit (except the commit message contains "[skip ci]"), a linting and testing pipeline is created automatically to make sure
the project still works as intended and follows previously defined code conventions.

The CI uses multiple bash and python scripts each designed for one specific purpose. Those scripts can be found in the `shell/` folder.

## Structure

The CI implementation of this Repository features two different types of pipelines, one with three and one with four stages.

##### Training Pipeline (Four stages, one job each)
###### Job descriptions
The first stage uses a shell environment to set up a Docker Image the next jobs can be executed on. This Image has all necessary dependancies including the plf-util Python Package already installed and is only built, if its configuration (Dockerfile) or the dependancies change or there is no saved Image (e.g. first run).

The second stage uses the Docker Image created in the first stage and runs the fc_train.py script. The model file is saved as a GitLab Artifact under ./oracles. There are three ways to tell this stage, which station's data to perform the training on (see Usage)

The third stage uses the second stage's job's artifact and runs an evaluation on it. The output is in turn stored as an artifact in ./oracles. Its behaviour is adaptive to the second stage, making sure the evaluation happens for the station the model was trained for.

The fourth stage features the `push_to_oracles` job. Its job is to fetch the model and evaluation files generated by previous jobs, structure them
in an intuitive folder structure and push to the `plf-oracles` Repository. Afterwards, this job triggers the pipeline of `plf-docs`, which in turn fetches `plf-oracles` with all its newly added results and includes them in the website under the `Results` tab. Each result page has a link back to its source pipeline and `push_to_oracles` also safes the link to the corresponding result page as an Artifact.

###### Used scripts
The training pipeline utilizes multiple scripts found in the `shell/` folder. Those scripts and their uses are explained here:

|Script Name|Function|
|---|---|
|trigger_training.sh|Checks if the $STATION variable is defined and executes fc_train.py if so|
|execute_evaluation.sh||
|||


## Requirements

To be able to  use this CI implementation when forking, there have to be configured runners with a `shell` and a `docker` environment accessible to the Repository. The tags assigned to these runners should also be `shell` and `docker`.


## Usage

There are three ways to interact with the CI implementation:

* By automatically detecting file changes [Removed]
* Using the commit message when pushing to remote
* By using the GitLab API

##### automatically detecting file changes [Removed]
The CI is set up in a way that it automatically detects changes to the config.json files of the nine included Gefcom2017 stations. If one or multiple config.json files are changed and pushed to remote, training and evaluation automatically starts for the corresponding stations. This ignores anything specified within the commit message of the push event, yet only happens when a push event is detected. When triggering a pipeline using GitLab's API, file changes are ignored.

##### using the commit message
If the pipeline is triggered by a push event, but no file changes to any config.json files are detected, the CI tries to extract the necessary path to a station config from the commit message. Similarly to `[ci skip]` or `[skip ci]`, that skips the CI completely if included in the commit message, `[<path to config>]` can be used to specify this path (inside of /targets). If the commit message does not include a set of `[]`, no pipeline will be created (same as using [ci skip]).

##### using the GitLab API
When triggering a pipeline by using GitLab's API, the config path has to be specified as a variable with the name `STATION` when performing the call. Triggering a pipeline using the API technically reruns the previous commit's pipeline, but overrides all other methods of interaction: If the last commit's message is `"changed something [ri]"` and it contains a change to `targets/gefcom2017/mass_data/config.json`, but its pipeline is rerun by an API call with `STATION = gefcom2017/ct_data`, training and evaluation is **only** performed for ct_data. Such an API call could look like this:

```bash
curl -X POST \
     -F token=TOKEN \
     -F "ref=REF_NAME" \
     -F "variables[STATION]=gefcom2017/mass_data" \
     https://your-gitlab-instance.com/api/v4/projects/your-project-id/trigger/pipeline
```
The Token can be aquired under `Settings/CI_CD/Pipeline_Triggers`. GitLab's API offers far more than just triggering pipelines, in fact it can perform every action that can be performed manually using the Web-UI. For further reading look up [GitLab's API Documentation](https://docs.gitlab.com/ee/api/README.html).
